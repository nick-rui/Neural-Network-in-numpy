We create a basic neural network for classification using only NumPy.

The model consists of an arbitrary number of linear layers, each using a rectified linear unit (ReLU) activation function with the last layer using a softmax activation.

The model trains on a cross-entropy loss function and a basic gradient descent algorithm.

The math behind the backpropagation is described in detail in the paper titled "Backpropagation Calculus".
